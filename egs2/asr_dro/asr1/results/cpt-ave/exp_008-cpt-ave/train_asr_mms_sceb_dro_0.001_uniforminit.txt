slurm submission log: 2024-08-15 21:53:51.028353
created following sbatch script: 

###############################

#!/bin/bash

#SBATCH --account=nlp
#SBATCH --cpus-per-task=2
#SBATCH --gres=gpu:a6000:1
#SBATCH --job-name=train_asr_mms_sceb_dro_0.001_uniforminit
#SBATCH --mail-type=ALL
#SBATCH --mail-user=bartelds@stanford.edu
#SBATCH --mem=16G
#SBATCH --open-mode=append
#SBATCH --output=results/exp_008/train_asr_mms_sceb_dro_0.001_uniforminit.txt
#SBATCH --partition=jag-standard
#SBATCH --time=14-0

# activate your desired anaconda environment
. /nlp/scr/bartelds/miniconda3/envs/asr-dro/etc/profile.d/conda.sh ; conda activate /nlp/scr/bartelds/miniconda3/envs/asr-dro

# cd to working directory
cd .

# launch commands
srun --unbuffered run_as_child_processes 'source ../../../tools/activate_python.sh; make train_asr_mms_sceb_dro_0.001_uniforminit'

###############################

submission to slurm complete!


###############################
slurm submission output

Submitted batch job 8133547



###############################

###############################
start time: 2024-08-15 21:53:53.524892
machine: jagupard34.stanford.edu
conda env: asr-dro
###############################
running following processes

	source ../../../tools/activate_python.sh; make train_asr_mms_sceb_dro_0.001_uniforminit


###############################
command outputs: 


/bin/sh: 1: source: not found
./run_multi.sh --duration 1h --lid true --only_lid false --dumpdir /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_008 --expdir /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008 --asr_stats_dir /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008 --specific_lang true --selected_languages afr,eng,spa,sah,nso,tgk,ast,ind,jav,tel,bre,som,isl,urd,kam --datasets nchlt,LAD,mls,commonvoice,nchlt,fleurs,fleurs,commonvoice,googlei18n_asr,fleurs,commonvoice,fleurs,fleurs,fleurs,fleurs --stage 11 --asr_tag train_asr_mms_sceb_dro_0.001_uniforminit --asr_config conf/exp_008/train_asr_mms_sceb_dro_0.001_uniforminit.yaml --batch_type language 
2024-08-15T21:53:53 (asr.sh:284:main) ./asr.sh --ngpu 1 --stage 11 --stop_stage 13 --nj 32 --inference_nj 4 --gpu_inference true --lang multilingual_1h__lid --inference_asr_model valid.loss.ave.pth --local_data_opts --duration 1h --lid true --only_lid false --multilingual true --nlsyms_txt data/local/nlsyms.txt --specific_lang true --selected_languages afr,eng,spa,sah,nso,tgk,ast,ind,jav,tel,bre,som,isl,urd,kam --datasets nchlt,LAD,mls,commonvoice,nchlt,fleurs,fleurs,commonvoice,googlei18n_asr,fleurs,commonvoice,fleurs,fleurs,fleurs,fleurs --nlsyms_txt data/local/nlsyms.txt --use_lm false --token_type char --feats_type raw --feats_normalize utterance_mvn --asr_config conf/exp_008/train_asr_mms_sceb_dro_0.001_uniforminit.yaml --inference_config conf/decode_asr.yaml --train_set train_1h_lid --valid_set dev_1h_lid --test_sets dev_1h_lid test_1h_lid --asr_tag train_asr_mms_sceb_dro_0.001_uniforminit --asr_stats_dir exp/asr_stats_multilingual_1h --local_score_opts true false normal --dumpdir /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_008 --expdir /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008 --asr_stats_dir /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008 --batch_type language
2024-08-15T21:53:53 (asr.sh:322:main) Info: The valid_set 'dev_1h_lid' is included in the test_sets. '--eval_valid_set true' is set and 'dev_1h_lid' is removed from the test_sets
2024-08-15T21:53:53 (asr.sh:566:main) Skipped stages:  6 7 8 9 14 15 16 
2024-08-15T21:53:53 (asr.sh:1310:main) Stage 11: ASR Training: train_set=/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_008/raw/train_1h_lid, valid_set=/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_008/raw/dev_1h_lid
2024-08-15T21:53:53 (asr.sh:1409:main) Generate '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008/asr_train_asr_mms_sceb_dro_0.001_uniforminit/run.sh'. You can resume the process from stage 11 using this script
2024-08-15T21:53:53 (asr.sh:1413:main) ASR training started... log: '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008/asr_train_asr_mms_sceb_dro_0.001_uniforminit/train.log'
2024-08-15 21:53:54,210 (launch:94) INFO: /nlp/scr/bartelds/miniconda3/envs/asr-dro/bin/python3 /juice2/scr2/bartelds/git/asr-dro/espnet/espnet2/bin/launch.py --cmd 'run.pl --name /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008/asr_train_asr_mms_sceb_dro_0.001_uniforminit/train.log' --log /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008/asr_train_asr_mms_sceb_dro_0.001_uniforminit/train.log --ngpu 1 --num_nodes 1 --init_file_prefix /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008/asr_train_asr_mms_sceb_dro_0.001_uniforminit/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel none --token_type char --token_list data/multilingual_1h__lid_token_list/char/tokens.txt --non_linguistic_symbols data/local/nlsyms.txt --cleaner none --g2p none --valid_data_path_and_name_and_type /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_008/raw/dev_1h_lid/wav.scp,speech,sound --valid_shape_file /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008/asr_train_asr_mms_sceb_dro_0.001_uniforminit --batch_type language --config conf/exp_008/train_asr_mms_sceb_dro_0.001_uniforminit.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_008/raw/train_1h_lid/wav.scp,speech,sound --train_shape_file /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_008/raw/train_1h_lid/text,text,text --train_shape_file /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008/train/text_shape.char --valid_data_path_and_name_and_type /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_008/raw/dev_1h_lid/text,text,text --valid_shape_file /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008/valid/text_shape.char
2024-08-15 21:53:54,352 (launch:348) INFO: log file: /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008/asr_train_asr_mms_sceb_dro_0.001_uniforminit/train.log
run.pl: job failed, log is in /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008/asr_train_asr_mms_sceb_dro_0.001_uniforminit/train.log
Command '['run.pl', '--name', '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008/asr_train_asr_mms_sceb_dro_0.001_uniforminit/train.log', '--gpu', '1', '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008/asr_train_asr_mms_sceb_dro_0.001_uniforminit/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'none', '--token_type', 'char', '--token_list', 'data/multilingual_1h__lid_token_list/char/tokens.txt', '--non_linguistic_symbols', 'data/local/nlsyms.txt', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_008/raw/dev_1h_lid/wav.scp,speech,sound', '--valid_shape_file', '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008/asr_train_asr_mms_sceb_dro_0.001_uniforminit', '--batch_type', 'language', '--config', 'conf/exp_008/train_asr_mms_sceb_dro_0.001_uniforminit.yaml', '--frontend_conf', 'fs=16k', '--train_data_path_and_name_and_type', '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_008/raw/train_1h_lid/wav.scp,speech,sound', '--train_shape_file', '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_008/raw/train_1h_lid/text,text,text', '--train_shape_file', '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008/train/text_shape.char', '--valid_data_path_and_name_and_type', '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_008/raw/dev_1h_lid/text,text,text', '--valid_shape_file', '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008/valid/text_shape.char', '--ngpu', '1', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/nlp/scr/bartelds/miniconda3/envs/asr-dro/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/nlp/scr/bartelds/miniconda3/envs/asr-dro/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/juice2/scr2/bartelds/git/asr-dro/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/juice2/scr2/bartelds/git/asr-dro/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008/asr_train_asr_mms_sceb_dro_0.001_uniforminit/train.log ###################
[jagupard34] 2024-08-15 21:54:26,195 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.2.final_layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,195 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.3.attention.k_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,195 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.3.attention.k_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,195 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.3.attention.v_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,195 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.3.attention.v_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,195 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.3.attention.q_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,195 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.3.attention.q_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,195 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.3.attention.out_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,195 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.3.attention.out_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,195 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.3.layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,195 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.3.layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,195 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.3.feed_forward.intermediate_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,195 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.3.feed_forward.intermediate_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,195 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.3.feed_forward.output_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,195 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.3.feed_forward.output_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,195 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.3.final_layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,195 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.3.final_layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,195 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.4.attention.k_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,195 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.4.attention.k_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,195 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.4.attention.v_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,195 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.4.attention.v_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.4.attention.q_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.4.attention.q_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.4.attention.out_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.4.attention.out_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.4.layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.4.layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.4.feed_forward.intermediate_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.4.feed_forward.intermediate_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.4.feed_forward.output_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.4.feed_forward.output_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.4.final_layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.4.final_layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.5.attention.k_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.5.attention.k_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.5.attention.v_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.5.attention.v_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.5.attention.q_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.5.attention.q_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.5.attention.out_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.5.attention.out_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.5.layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.5.layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.5.feed_forward.intermediate_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.5.feed_forward.intermediate_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.5.feed_forward.output_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.5.feed_forward.output_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.5.final_layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.5.final_layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.6.attention.k_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.6.attention.k_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.6.attention.v_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.6.attention.v_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.6.attention.q_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.6.attention.q_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.6.attention.out_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.6.attention.out_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.6.layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.6.layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.6.feed_forward.intermediate_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.6.feed_forward.intermediate_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.6.feed_forward.output_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.6.feed_forward.output_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.6.final_layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.6.final_layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.7.attention.k_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.7.attention.k_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.7.attention.v_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.7.attention.v_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.7.attention.q_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.7.attention.q_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.7.attention.out_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.7.attention.out_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.7.layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.7.layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.7.feed_forward.intermediate_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.7.feed_forward.intermediate_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.7.feed_forward.output_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,196 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.7.feed_forward.output_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.7.final_layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.7.final_layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.8.attention.k_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.8.attention.k_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.8.attention.v_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.8.attention.v_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.8.attention.q_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.8.attention.q_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.8.attention.out_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.8.attention.out_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.8.layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.8.layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.8.feed_forward.intermediate_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.8.feed_forward.intermediate_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.8.feed_forward.output_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.8.feed_forward.output_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.8.final_layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.8.final_layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.9.attention.k_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.9.attention.k_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.9.attention.v_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.9.attention.v_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.9.attention.q_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.9.attention.q_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.9.attention.out_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.9.attention.out_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.9.layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.9.layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.9.feed_forward.intermediate_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.9.feed_forward.intermediate_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.9.feed_forward.output_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.9.feed_forward.output_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.9.final_layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.9.final_layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.10.attention.k_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.10.attention.k_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.10.attention.v_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.10.attention.v_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.10.attention.q_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.10.attention.q_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.10.attention.out_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.10.attention.out_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.10.layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.10.layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.10.feed_forward.intermediate_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.10.feed_forward.intermediate_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.10.feed_forward.output_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.10.feed_forward.output_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.10.final_layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.10.final_layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.11.attention.k_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.11.attention.k_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.11.attention.v_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.11.attention.v_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.11.attention.q_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.11.attention.q_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,197 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.11.attention.out_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.11.attention.out_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.11.layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.11.layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.11.feed_forward.intermediate_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.11.feed_forward.intermediate_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.11.feed_forward.output_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.11.feed_forward.output_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.11.final_layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.11.final_layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.12.attention.k_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.12.attention.k_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.12.attention.v_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.12.attention.v_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.12.attention.q_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.12.attention.q_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.12.attention.out_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.12.attention.out_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.12.layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.12.layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.12.feed_forward.intermediate_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.12.feed_forward.intermediate_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.12.feed_forward.output_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.12.feed_forward.output_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.12.final_layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.12.final_layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.13.attention.k_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.13.attention.k_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.13.attention.v_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.13.attention.v_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.13.attention.q_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.13.attention.q_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.13.attention.out_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.13.attention.out_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.13.layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.13.layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.13.feed_forward.intermediate_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.13.feed_forward.intermediate_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.13.feed_forward.output_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.13.feed_forward.output_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.13.final_layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.13.final_layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.attention.k_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.attention.k_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.attention.v_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.attention.v_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.attention.q_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.attention.q_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.attention.out_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.attention.out_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.feed_forward.intermediate_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.feed_forward.intermediate_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.feed_forward.output_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.feed_forward.output_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.final_layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.14.final_layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,198 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.attention.k_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.attention.k_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.attention.v_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.attention.v_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.attention.q_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.attention.q_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.attention.out_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.attention.out_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.feed_forward.intermediate_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.feed_forward.intermediate_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.feed_forward.output_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.feed_forward.output_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.final_layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.15.final_layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.attention.k_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.attention.k_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.attention.v_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.attention.v_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.attention.q_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.attention.q_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.attention.out_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.attention.out_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.feed_forward.intermediate_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.feed_forward.intermediate_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.feed_forward.output_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.feed_forward.output_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.final_layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.16.final_layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.attention.k_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.attention.k_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.attention.v_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.attention.v_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.attention.q_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.attention.q_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.attention.out_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.attention.out_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.feed_forward.intermediate_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.feed_forward.intermediate_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.feed_forward.output_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.feed_forward.output_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.final_layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.17.final_layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.attention.k_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.attention.k_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.attention.v_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.attention.v_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.attention.q_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.attention.q_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.attention.out_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.attention.out_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.feed_forward.intermediate_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,199 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.feed_forward.intermediate_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.feed_forward.output_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.feed_forward.output_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.final_layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.18.final_layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.attention.k_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.attention.k_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.attention.v_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.attention.v_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.attention.q_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.attention.q_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.attention.out_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.attention.out_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.feed_forward.intermediate_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.feed_forward.intermediate_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.feed_forward.output_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.feed_forward.output_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.final_layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.19.final_layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.attention.k_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.attention.k_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.attention.v_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.attention.v_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.attention.q_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.attention.q_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.attention.out_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.attention.out_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.feed_forward.intermediate_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.feed_forward.intermediate_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.feed_forward.output_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.feed_forward.output_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.final_layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.20.final_layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.attention.k_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.attention.k_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.attention.v_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.attention.v_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.attention.q_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.attention.q_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.attention.out_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.attention.out_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.feed_forward.intermediate_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.feed_forward.intermediate_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.feed_forward.output_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.feed_forward.output_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.final_layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.21.final_layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.attention.k_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.attention.k_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.attention.v_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.attention.v_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.attention.q_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.attention.q_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.attention.out_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,200 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.attention.out_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,201 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,201 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,201 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.feed_forward.intermediate_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,201 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.feed_forward.intermediate_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,201 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.feed_forward.output_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,201 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.feed_forward.output_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,201 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.final_layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,201 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.22.final_layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,201 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.attention.k_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,201 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.attention.k_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,201 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.attention.v_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,201 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.attention.v_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,201 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.attention.q_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,201 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.attention.q_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,201 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.attention.out_proj.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,201 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.attention.out_proj.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,201 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,201 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,201 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.feed_forward.intermediate_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,201 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.feed_forward.intermediate_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,201 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.feed_forward.output_dense.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,201 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.feed_forward.output_dense.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,201 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.final_layer_norm.weight.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,201 (abs_task:1280) INFO: Setting frontend.upstream.upstream.model.encoder.layers.23.final_layer_norm.bias.requires_grad = False
[jagupard34] 2024-08-15 21:54:26,636 (abs_task:1308) INFO: pytorch.version=2.1.0, cuda.available=True, cudnn.version=8902, cudnn.benchmark=False, cudnn.deterministic=True
[jagupard34] 2024-08-15 21:54:26,642 (abs_task:1309) INFO: Model structure:
ESPnetASRModel(
  (frontend): S3prlFrontend(
    (upstream): S3PRLUpstream(
      (upstream): UpstreamExpert(
        (model): Wav2Vec2Model(
          (feature_extractor): Wav2Vec2FeatureEncoder(
            (conv_layers): ModuleList(
              (0): Wav2Vec2LayerNormConvLayer(
                (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))
                (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (activation): GELUActivation()
              )
              (1-4): 4 x Wav2Vec2LayerNormConvLayer(
                (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))
                (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (activation): GELUActivation()
              )
              (5-6): 2 x Wav2Vec2LayerNormConvLayer(
                (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))
                (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
                (activation): GELUActivation()
              )
            )
          )
          (feature_projection): Wav2Vec2FeatureProjection(
            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (projection): Linear(in_features=512, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (encoder): Wav2Vec2EncoderStableLayerNorm(
            (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(
              (conv): ParametrizedConv1d(
                1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16
                (parametrizations): ModuleDict(
                  (weight): ParametrizationList(
                    (0): _WeightNorm()
                  )
                )
              )
              (padding): Wav2Vec2SamePadLayer()
              (activation): GELUActivation()
            )
            (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (layers): ModuleList(
              (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(
                (attention): Wav2Vec2Attention(
                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
                )
                (dropout): Dropout(p=0.1, inplace=False)
                (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
                (feed_forward): Wav2Vec2FeedForward(
                  (intermediate_dropout): Dropout(p=0.0, inplace=False)
                  (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)
                  (intermediate_act_fn): GELUActivation()
                  (output_dense): Linear(in_features=4096, out_features=1024, bias=True)
                  (output_dropout): Dropout(p=0.1, inplace=False)
                )
                (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
        )
      )
    )
    (featurizer): Featurizer()
  )
  (specaug): SpecAug(
    (time_warp): TimeWarp(window=5, mode=bicubic)
    (freq_mask): MaskAlongAxis(mask_width_range=[0, 27], num_mask=2, axis=freq)
    (time_mask): MaskAlongAxisVariableMaxWidth(mask_width_ratio_range=[0.0, 0.05], num_mask=10, axis=time)
  )
  (normalize): UtteranceMVN(norm_means=True, norm_vars=False)
  (preencoder): LinearProjection(
    (linear_out): Linear(in_features=1024, out_features=80, bias=True)
    (dropout): Dropout(p=0.0, inplace=False)
  )
  (encoder): TransformerEncoder(
    (embed): Conv2dSubsampling2(
      (conv): Sequential(
        (0): Conv2d(1, 512, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=18944, out_features=512, bias=True)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (4): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (5): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (6): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (7): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (8): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (9): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (10): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (11): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (12): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (13): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (14): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (15): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (16): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (17): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (18): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (19): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (20): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (21): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (22): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (23): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (activation): ReLU()
        )
        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
  )
  (criterion_att): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=512, out_features=250, bias=True)
    (ctc_loss): DROCTCLoss()
  )
)

Model summary:
    Class Name: ESPnetASRModel
    Total Number of model parameters: 403.37 M
    Number of trainable parameters: 87.93 M (21.8%)
    Size: 351.73 MB
    Type: torch.float32
[jagupard34] 2024-08-15 21:54:26,642 (abs_task:1312) INFO: Optimizer:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.001
)
[jagupard34] 2024-08-15 21:54:26,642 (abs_task:1313) INFO: Scheduler: None
[jagupard34] 2024-08-15 21:54:26,643 (abs_task:1322) INFO: Saving the configuration in /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008/asr_train_asr_mms_sceb_dro_0.001_uniforminit/config.yaml
[jagupard34] 2024-08-15 21:54:26,665 (asr:493) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[jagupard34] 2024-08-15 21:54:26,665 (abs_task:1691) WARNING: Reading /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_008/raw/train_1h_lid/utt2category
[jagupard34] 2024-08-15 21:54:26,677 (abs_task:1729) INFO: [train] dataset:
ESPnetDataset(
  speech: {"path": "/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_008/raw/train_1h_lid/wav.scp", "type": "sound"}
  text: {"path": "/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_008/raw/train_1h_lid/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7f2235213460>)
[jagupard34] 2024-08-15 21:54:26,677 (abs_task:1730) INFO: [train] Batch sampler: LanguageBatchSampler(N-batch=1010, batch_size=8, key_file=/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008/train/speech_shape, 
[jagupard34] 2024-08-15 21:54:26,677 (abs_task:1731) INFO: [train] mini-batch sizes summary: N-batch=1010, mean=8.0, min=8, max=8
[jagupard34] 2024-08-15 21:54:26,681 (asr:493) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[jagupard34] 2024-08-15 21:54:26,681 (abs_task:1691) WARNING: Reading /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_008/raw/dev_1h_lid/utt2category
[jagupard34] 2024-08-15 21:54:26,684 (abs_task:1729) INFO: [valid] dataset:
ESPnetDataset(
  speech: {"path": "/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_008/raw/dev_1h_lid/wav.scp", "type": "sound"}
  text: {"path": "/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_008/raw/dev_1h_lid/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7f22374fd820>)
[jagupard34] 2024-08-15 21:54:26,684 (abs_task:1730) INFO: [valid] Batch sampler: LanguageBatchSampler(N-batch=180, batch_size=8, key_file=/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008/valid/speech_shape, 
[jagupard34] 2024-08-15 21:54:26,684 (abs_task:1731) INFO: [valid] mini-batch sizes summary: N-batch=180, mean=8.0, min=8, max=8
[jagupard34] 2024-08-15 21:54:26,691 (asr:493) INFO: Optional Data Names: ('text_spk2', 'text_spk3', 'text_spk4', 'prompt')
[jagupard34] 2024-08-15 21:54:26,691 (abs_task:1691) WARNING: Reading /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_008/raw/dev_1h_lid/utt2category
[jagupard34] 2024-08-15 21:54:26,693 (abs_task:1729) INFO: [plot_att] dataset:
ESPnetDataset(
  speech: {"path": "/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_008/raw/dev_1h_lid/wav.scp", "type": "sound"}
  text: {"path": "/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_008/raw/dev_1h_lid/text", "type": "text"}
  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7f2235043790>)
[jagupard34] 2024-08-15 21:54:26,693 (abs_task:1730) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=1402, batch_size=1, key_file=/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008/valid/speech_shape, 
[jagupard34] 2024-08-15 21:54:26,693 (abs_task:1731) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1
wandb: Currently logged in as: bartelds. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.6
wandb: Run data is saved locally in /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008/asr_train_asr_mms_sceb_dro_0.001_uniforminit/wandb/run-20240815_215427-x0fd2le6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run _juice2_scr2_bartelds_git_asr-dro_espnet_egs2_asr_dro_asr1
wandb:  View project at https://wandb.ai/bartelds/asr-dro-dro-train-large-bs
wandb:  View run at https://wandb.ai/bartelds/asr-dro-dro-train-large-bs/runs/x0fd2le6
[jagupard34] 2024-08-15 21:54:30,709 (trainer:311) INFO: 1/30epoch started
/nlp/scr/bartelds/miniconda3/envs/asr-dro/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
utt2category_file /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_008/raw/train_1h_lid/utt2category
/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_008/raw/train_1h_lid
utt2category_file /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_008/raw/dev_1h_lid/utt2category
/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_008/raw/dev_1h_lid
Traceback (most recent call last):
  File "/nlp/scr/bartelds/miniconda3/envs/asr-dro/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/nlp/scr/bartelds/miniconda3/envs/asr-dro/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/juice2/scr2/bartelds/git/asr-dro/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/juice2/scr2/bartelds/git/asr-dro/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/juice2/scr2/bartelds/git/asr-dro/espnet/espnet2/tasks/abs_task.py", line 1157, in main
    cls.main_worker(args)
  File "/juice2/scr2/bartelds/git/asr-dro/espnet/espnet2/tasks/abs_task.py", line 1487, in main_worker
    cls.trainer.run(
  File "/juice2/scr2/bartelds/git/asr-dro/espnet/espnet2/train/trainer.py", line 317, in run
    all_steps_are_invalid = cls.train_one_epoch(
  File "/juice2/scr2/bartelds/git/asr-dro/espnet/espnet2/train/trainer.py", line 615, in train_one_epoch
    retval = model(**batch)
  File "/nlp/scr/bartelds/miniconda3/envs/asr-dro/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/nlp/scr/bartelds/miniconda3/envs/asr-dro/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/juice2/scr2/bartelds/git/asr-dro/espnet/espnet2/asr/espnet_model.py", line 238, in forward
    encoder_out, encoder_out_lens = self.encode(speech, speech_lengths)
  File "/juice2/scr2/bartelds/git/asr-dro/espnet/espnet2/asr/espnet_model.py", line 404, in encode
    encoder_out, encoder_out_lens, _ = self.encoder(feats, feats_lengths)
  File "/nlp/scr/bartelds/miniconda3/envs/asr-dro/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/nlp/scr/bartelds/miniconda3/envs/asr-dro/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/juice2/scr2/bartelds/git/asr-dro/espnet/espnet2/asr/encoder/transformer_encoder.py", line 224, in forward
    xs_pad, masks = encoder_layer(xs_pad, masks)
  File "/nlp/scr/bartelds/miniconda3/envs/asr-dro/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/nlp/scr/bartelds/miniconda3/envs/asr-dro/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/juice2/scr2/bartelds/git/asr-dro/espnet/espnet/nets/pytorch_backend/transformer/encoder_layer.py", line 104, in forward
    self.self_attn(x_q, x, x, mask)
  File "/nlp/scr/bartelds/miniconda3/envs/asr-dro/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/nlp/scr/bartelds/miniconda3/envs/asr-dro/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/juice2/scr2/bartelds/git/asr-dro/espnet/espnet/nets/pytorch_backend/transformer/attention.py", line 110, in forward
    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacty of 47.54 GiB of which 31.88 MiB is free. Including non-PyTorch memory, this process has 47.50 GiB memory in use. Of the allocated memory 46.06 GiB is allocated by PyTorch, and 1.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: - 0.013 MB of 0.037 MB uploaded
wandb: \ 0.037 MB of 0.037 MB uploaded
wandb:  View run _juice2_scr2_bartelds_git_asr-dro_espnet_egs2_asr_dro_asr1 at: https://wandb.ai/bartelds/asr-dro-dro-train-large-bs/runs/x0fd2le6
wandb:  View project at: https://wandb.ai/bartelds/asr-dro-dro-train-large-bs
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_008/asr_train_asr_mms_sceb_dro_0.001_uniforminit/wandb/run-20240815_215427-x0fd2le6/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[0, 0, 0, 0, 0, 0, 0, 0]
normalized dro_q:
q[group#nso]= 0.06666666269302368
# Accounting: time=46 threads=1
# Ended (code 1) at Thu Aug 15 21:54:40 PDT 2024, elapsed time 46 seconds

make: *** [exp008.mk:190: train_asr_mms_sceb_dro_0.001_uniforminit] Error 1
###############################
end time: 2024-08-15 21:54:43.569242
elapsed time: 0:00:50.044350
