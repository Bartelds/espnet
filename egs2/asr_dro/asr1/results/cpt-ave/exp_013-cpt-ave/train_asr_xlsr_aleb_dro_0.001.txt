slurm submission log: 2024-08-23 08:39:33.599348
created following sbatch script: 

###############################

#!/bin/bash

#SBATCH --account=nlp
#SBATCH --cpus-per-task=2
#SBATCH --gres=gpu:a6000:1
#SBATCH --job-name=train_asr_xlsr_aleb_dro_0.001
#SBATCH --mail-type=ALL
#SBATCH --mail-user=bartelds@stanford.edu
#SBATCH --mem=16G
#SBATCH --open-mode=append
#SBATCH --output=results/exp_013/train_asr_xlsr_aleb_dro_0.001.txt
#SBATCH --partition=jag-standard
#SBATCH --time=14-0

# activate your desired anaconda environment
. /nlp/scr/bartelds/miniconda3/envs/asr-dro/etc/profile.d/conda.sh ; conda activate /nlp/scr/bartelds/miniconda3/envs/asr-dro

# cd to working directory
cd .

# launch commands
srun --unbuffered run_as_child_processes 'source ../../../tools/activate_python.sh; make train_asr_xlsr_aleb_dro_0.001'

###############################

submission to slurm complete!


###############################
slurm submission output

Submitted batch job 8255325



###############################

###############################
start time: 2024-08-23 08:39:37.060204
machine: jagupard32.stanford.edu
conda env: asr-dro
###############################
running following processes

	source ../../../tools/activate_python.sh; make train_asr_xlsr_aleb_dro_0.001


###############################
command outputs: 


/bin/sh: 1: source: not found
./run_multi.sh --duration 1h --lid true --only_lid false --dumpdir /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_013 --expdir /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013 --asr_stats_dir /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013 --specific_lang true --selected_languages nld,hrv,pol,slk,deu,ita,swa,myv,fra,ben,asm,lao,cym,urd,kir --datasets commonvoice,fleurs,M-AILABS,commonvoice,M-AILABS,commonvoice,commonvoice,commonvoice,fleurs,fleurs,commonvoice,fleurs,commonvoice,commonvoice,commonvoice --stage 11 --asr_tag train_asr_xlsr_aleb_dro_0.001 --asr_config conf/exp_013/train_asr_xlsr_aleb_dro_0.001.yaml --batch_type duration_language 
2024-08-23T08:39:37 (asr.sh:284:main) ./asr.sh --ngpu 1 --stage 11 --stop_stage 13 --nj 32 --inference_nj 4 --gpu_inference true --lang multilingual_1h__lid --inference_asr_model valid.loss.ave.pth --local_data_opts --duration 1h --lid true --only_lid false --multilingual true --nlsyms_txt data/local/nlsyms.txt --specific_lang true --selected_languages nld,hrv,pol,slk,deu,ita,swa,myv,fra,ben,asm,lao,cym,urd,kir --datasets commonvoice,fleurs,M-AILABS,commonvoice,M-AILABS,commonvoice,commonvoice,commonvoice,fleurs,fleurs,commonvoice,fleurs,commonvoice,commonvoice,commonvoice --nlsyms_txt data/local/nlsyms.txt --use_lm false --token_type char --feats_type raw --feats_normalize utterance_mvn --asr_config conf/exp_013/train_asr_xlsr_aleb_dro_0.001.yaml --inference_config conf/decode_asr.yaml --train_set train_1h_lid --valid_set dev_1h_lid --test_sets dev_1h_lid test_1h_lid --asr_tag train_asr_xlsr_aleb_dro_0.001 --asr_stats_dir exp/asr_stats_multilingual_1h --local_score_opts true false normal --dumpdir /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_013 --expdir /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013 --asr_stats_dir /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013 --batch_type duration_language
2024-08-23T08:39:37 (asr.sh:322:main) Info: The valid_set 'dev_1h_lid' is included in the test_sets. '--eval_valid_set true' is set and 'dev_1h_lid' is removed from the test_sets
2024-08-23T08:39:37 (asr.sh:566:main) Skipped stages:  6 7 8 9 14 15 16 
2024-08-23T08:39:37 (asr.sh:1310:main) Stage 11: ASR Training: train_set=/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_013/raw/train_1h_lid, valid_set=/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_013/raw/dev_1h_lid
2024-08-23T08:39:37 (asr.sh:1409:main) Generate '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013/asr_train_asr_xlsr_aleb_dro_0.001/run.sh'. You can resume the process from stage 11 using this script
2024-08-23T08:39:37 (asr.sh:1413:main) ASR training started... log: '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013/asr_train_asr_xlsr_aleb_dro_0.001/train.log'
2024-08-23 08:39:38,879 (launch:94) INFO: /nlp/scr/bartelds/miniconda3/envs/asr-dro/bin/python3 /juice2/scr2/bartelds/git/asr-dro/espnet/espnet2/bin/launch.py --cmd 'run.pl --name /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013/asr_train_asr_xlsr_aleb_dro_0.001/train.log' --log /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013/asr_train_asr_xlsr_aleb_dro_0.001/train.log --ngpu 1 --num_nodes 1 --init_file_prefix /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013/asr_train_asr_xlsr_aleb_dro_0.001/.dist_init_ --multiprocessing_distributed true -- python3 -m espnet2.bin.asr_train --use_preprocessor true --bpemodel none --token_type char --token_list data/multilingual_1h__lid_token_list/char/tokens.txt --non_linguistic_symbols data/local/nlsyms.txt --cleaner none --g2p none --valid_data_path_and_name_and_type /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_013/raw/dev_1h_lid/wav.scp,speech,sound --valid_shape_file /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013/valid/speech_shape --resume true --ignore_init_mismatch false --fold_length 80000 --output_dir /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013/asr_train_asr_xlsr_aleb_dro_0.001 --batch_type duration_language --config conf/exp_013/train_asr_xlsr_aleb_dro_0.001.yaml --frontend_conf fs=16k --train_data_path_and_name_and_type /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_013/raw/train_1h_lid/wav.scp,speech,sound --train_shape_file /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013/train/speech_shape --fold_length 150 --train_data_path_and_name_and_type /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_013/raw/train_1h_lid/text,text,text --train_shape_file /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013/train/text_shape.char --valid_data_path_and_name_and_type /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_013/raw/dev_1h_lid/text,text,text --valid_shape_file /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013/valid/text_shape.char
2024-08-23 08:39:39,044 (launch:348) INFO: log file: /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013/asr_train_asr_xlsr_aleb_dro_0.001/train.log
run.pl: job failed, log is in /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013/asr_train_asr_xlsr_aleb_dro_0.001/train.log
Command '['run.pl', '--name', '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013/asr_train_asr_xlsr_aleb_dro_0.001/train.log', '--gpu', '1', '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013/asr_train_asr_xlsr_aleb_dro_0.001/train.log', 'python3', '-m', 'espnet2.bin.asr_train', '--use_preprocessor', 'true', '--bpemodel', 'none', '--token_type', 'char', '--token_list', 'data/multilingual_1h__lid_token_list/char/tokens.txt', '--non_linguistic_symbols', 'data/local/nlsyms.txt', '--cleaner', 'none', '--g2p', 'none', '--valid_data_path_and_name_and_type', '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_013/raw/dev_1h_lid/wav.scp,speech,sound', '--valid_shape_file', '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013/valid/speech_shape', '--resume', 'true', '--ignore_init_mismatch', 'false', '--fold_length', '80000', '--output_dir', '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013/asr_train_asr_xlsr_aleb_dro_0.001', '--batch_type', 'duration_language', '--config', 'conf/exp_013/train_asr_xlsr_aleb_dro_0.001.yaml', '--frontend_conf', 'fs=16k', '--train_data_path_and_name_and_type', '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_013/raw/train_1h_lid/wav.scp,speech,sound', '--train_shape_file', '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013/train/speech_shape', '--fold_length', '150', '--train_data_path_and_name_and_type', '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_013/raw/train_1h_lid/text,text,text', '--train_shape_file', '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013/train/text_shape.char', '--valid_data_path_and_name_and_type', '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/dump/_exp_013/raw/dev_1h_lid/text,text,text', '--valid_shape_file', '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013/valid/text_shape.char', '--ngpu', '1', '--multiprocessing_distributed', 'True']' returned non-zero exit status 1.
Traceback (most recent call last):
  File "/nlp/scr/bartelds/miniconda3/envs/asr-dro/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/nlp/scr/bartelds/miniconda3/envs/asr-dro/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/juice2/scr2/bartelds/git/asr-dro/espnet/espnet2/bin/launch.py", line 384, in <module>
    main()
  File "/juice2/scr2/bartelds/git/asr-dro/espnet/espnet2/bin/launch.py", line 375, in main
    raise RuntimeError(
RuntimeError: 
################### The last 1000 lines of /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013/asr_train_asr_xlsr_aleb_dro_0.001/train.log ###################
q[group#ita]= 0.066408172249794
q[group#cym]= 0.06768182665109634
q[group#asm]= 0.0646071806550026
q[group#urd]= 0.06517446041107178
q[group#ben]= 0.06503575295209885
[5, 5, 5, 5]
normalized dro_q:
q[group#slk]= 0.062696173787117
q[group#swa]= 0.06553211808204651
q[group#nld]= 0.07156135886907578
q[group#deu]= 0.07221969217061996
q[group#kir]= 0.06367456912994385
q[group#fra]= 0.07529345154762268
q[group#lao]= 0.064942866563797
q[group#hrv]= 0.06465160101652145
q[group#myv]= 0.06372420489788055
q[group#pol]= 0.0668221265077591
q[group#ita]= 0.06640293449163437
q[group#cym]= 0.06767629832029343
q[group#asm]= 0.06460235267877579
q[group#urd]= 0.06516950577497482
q[group#ben]= 0.06503082811832428
[13, 13, 13, 13, 13, 13, 13, 13]
normalized dro_q:
q[group#slk]= 0.06269538402557373
q[group#swa]= 0.06553084403276443
q[group#nld]= 0.07155904918909073
q[group#deu]= 0.07221727073192596
q[group#kir]= 0.06367361545562744
q[group#fra]= 0.0752905011177063
q[group#lao]= 0.06494169682264328
q[group#hrv]= 0.06465047597885132
q[group#myv]= 0.06372324377298355
q[group#pol]= 0.06682062894105911
q[group#ita]= 0.06640151143074036
q[group#cym]= 0.0676746591925621
q[group#asm]= 0.06460123509168625
q[group#urd]= 0.06519026309251785
q[group#ben]= 0.06502964347600937
[2, 2, 2, 2, 2, 2]
normalized dro_q:
q[group#slk]= 0.06269406527280807
q[group#swa]= 0.06552901864051819
q[group#nld]= 0.07158657163381577
q[group#deu]= 0.07221423834562302
q[group#kir]= 0.06367212533950806
q[group#fra]= 0.07528690993785858
q[group#lao]= 0.06493997573852539
q[group#hrv]= 0.06464880704879761
q[group#myv]= 0.06372173875570297
q[group#pol]= 0.06681856513023376
q[group#ita]= 0.06639952957630157
q[group#cym]= 0.06767244637012482
q[group#asm]= 0.06459957361221313
q[group#urd]= 0.06518849730491638
q[group#ben]= 0.06502790749073029
[0, 0, 0, 0, 0, 0, 0, 0, 0]
normalized dro_q:
q[group#slk]= 0.06271210312843323
q[group#swa]= 0.06552796810865402
q[group#nld]= 0.07158450037240982
q[group#deu]= 0.07221206277608871
q[group#kir]= 0.06367138773202896
q[group#fra]= 0.07528422027826309
q[group#lao]= 0.06493902951478958
q[group#hrv]= 0.06464790552854538
q[group#myv]= 0.06372099369764328
q[group#pol]= 0.06681729853153229
q[group#ita]= 0.06639833748340607
q[group#cym]= 0.067671038210392
q[group#asm]= 0.0645986869931221
q[group#urd]= 0.06518750637769699
q[group#ben]= 0.06502694636583328
[1, 1, 1, 1, 1, 1]
normalized dro_q:
q[group#slk]= 0.06271126121282578
q[group#swa]= 0.06554951518774033
q[group#nld]= 0.0715821236371994
q[group#deu]= 0.07220958173274994
q[group#kir]= 0.06367038190364838
q[group#fra]= 0.07528120279312134
q[group#lao]= 0.06493780016899109
q[group#hrv]= 0.06464672833681107
q[group#myv]= 0.0637199804186821
q[group#pol]= 0.06681574881076813
q[group#ita]= 0.06639686226844788
q[group#cym]= 0.0676693394780159
q[group#asm]= 0.06459751725196838
q[group#urd]= 0.06518623977899551
q[group#ben]= 0.0650257021188736
[2, 2, 2, 2, 2, 2]
normalized dro_q:
q[group#slk]= 0.06270971149206161
q[group#swa]= 0.06554744392633438
q[group#nld]= 0.0716131329536438
q[group#deu]= 0.0722062811255455
q[group#kir]= 0.06366865336894989
q[group#fra]= 0.07527733594179153
q[group#lao]= 0.0649358406662941
q[group#hrv]= 0.06464482098817825
q[group#myv]= 0.06371824443340302
q[group#pol]= 0.06681343913078308
q[group#ita]= 0.0663946345448494
q[group#cym]= 0.06766687333583832
q[group#asm]= 0.06459561735391617
q[group#urd]= 0.06518423557281494
q[group#ben]= 0.06502372771501541
[2]
normalized dro_q:
q[group#slk]= 0.06270933896303177
q[group#swa]= 0.06554660201072693
q[group#nld]= 0.07162666320800781
q[group#deu]= 0.0722043439745903
q[group#kir]= 0.06366812437772751
q[group#fra]= 0.07527488470077515
q[group#lao]= 0.064935103058815
q[group#hrv]= 0.06464412808418274
q[group#myv]= 0.06371770799160004
q[group#pol]= 0.06681238859891891
q[group#ita]= 0.0663936585187912
q[group#cym]= 0.06766568124294281
q[group#asm]= 0.06459493935108185
q[group#urd]= 0.06518345326185226
q[group#ben]= 0.06502297520637512
[14, 14, 14]
normalized dro_q:
q[group#slk]= 0.06270800530910492
q[group#swa]= 0.06554475426673889
q[group#nld]= 0.07162372022867203
q[group#deu]= 0.07220129668712616
q[group#kir]= 0.06366661936044693
q[group#fra]= 0.07527127861976624
q[group#lao]= 0.06493336707353592
q[group#hrv]= 0.06464244425296783
q[group#myv]= 0.06371619552373886
q[group#pol]= 0.06681030988693237
q[group#ita]= 0.06639166176319122
q[group#cym]= 0.06766345351934433
q[group#asm]= 0.06459326297044754
q[group#urd]= 0.0651816725730896
q[group#ben]= 0.06505190581083298
[5, 5, 5, 5]
normalized dro_q:
q[group#slk]= 0.06270502507686615
q[group#swa]= 0.06554119288921356
q[group#nld]= 0.07161889970302582
q[group#deu]= 0.0721963569521904
q[group#kir]= 0.06366344541311264
q[group#fra]= 0.07532263547182083
q[group#lao]= 0.06492993235588074
q[group#hrv]= 0.06463906913995743
q[group#myv]= 0.06371300667524338
q[group#pol]= 0.06680648028850555
q[group#ita]= 0.06638792157173157
q[group#cym]= 0.06765945255756378
q[group#asm]= 0.06458989530801773
q[group#urd]= 0.06517818570137024
q[group#ben]= 0.06504844129085541
[0, 0, 0, 0, 0, 0, 0, 0]
normalized dro_q:
q[group#slk]= 0.06273001432418823
q[group#swa]= 0.06553965061903
q[group#nld]= 0.0716162919998169
q[group#deu]= 0.07219364494085312
q[group#kir]= 0.06366223841905594
q[group#fra]= 0.07531937211751938
q[group#lao]= 0.06492850184440613
q[group#hrv]= 0.0646376833319664
q[group#myv]= 0.06371178478002548
q[group#pol]= 0.06680471450090408
q[group#ita]= 0.06638623028993607
q[group#cym]= 0.06765753775835037
q[group#asm]= 0.0645885244011879
q[group#urd]= 0.06517671048641205
q[group#ben]= 0.06504698842763901
[9, 9]
normalized dro_q:
q[group#slk]= 0.06272900104522705
q[group#swa]= 0.06553813815116882
q[group#nld]= 0.07161371409893036
q[group#deu]= 0.07219097018241882
q[group#kir]= 0.06366106122732162
q[group#fra]= 0.07531614601612091
q[group#lao]= 0.0649271011352539
q[group#hrv]= 0.06463633477687836
q[group#myv]= 0.06371059268712997
q[group#pol]= 0.0668286681175232
q[group#ita]= 0.06638456881046295
q[group#cym]= 0.06765565276145935
q[group#asm]= 0.06458718329668045
q[group#urd]= 0.06517526507377625
q[group#ben]= 0.065045565366745
[13, 13, 13, 13, 13, 13, 13, 13]
normalized dro_q:
q[group#slk]= 0.06272827833890915
q[group#swa]= 0.06553693115711212
q[group#nld]= 0.07161147147417068
q[group#deu]= 0.07218863070011139
q[group#kir]= 0.06366017460823059
q[group#fra]= 0.0753132700920105
q[group#lao]= 0.06492599844932556
q[group#hrv]= 0.06463528424501419
q[group#myv]= 0.06370969861745834
q[group#pol]= 0.06682724505662918
q[group#ita]= 0.06638322025537491
q[group#cym]= 0.067654088139534
q[group#asm]= 0.06458614021539688
q[group#urd]= 0.06519514322280884
q[group#ben]= 0.06504444777965546
[0, 0, 0, 0, 0, 0, 0, 0]
normalized dro_q:
q[group#slk]= 0.06278049200773239
q[group#swa]= 0.06553349643945694
q[group#nld]= 0.07160678505897522
q[group#deu]= 0.07218383252620697
q[group#kir]= 0.06365711987018585
q[group#fra]= 0.07530783116817474
q[group#lao]= 0.06492268294095993
q[group#hrv]= 0.06463202834129333
q[group#myv]= 0.06370663642883301
q[group#pol]= 0.06682354211807251
q[group#ita]= 0.0663796067237854
q[group#cym]= 0.0676502138376236
q[group#asm]= 0.06458289921283722
q[group#urd]= 0.06519177556037903
q[group#ben]= 0.06504110991954803
[5, 5, 5, 5]
normalized dro_q:
q[group#slk]= 0.06277459114789963
q[group#swa]= 0.06552689522504807
q[group#nld]= 0.07159864902496338
q[group#deu]= 0.07217554748058319
q[group#kir]= 0.06365099549293518
q[group#fra]= 0.07540203630924225
q[group#lao]= 0.06491623818874359
q[group#hrv]= 0.06462565809488297
q[group#myv]= 0.06370049715042114
q[group#pol]= 0.06681661307811737
q[group#ita]= 0.06637278944253922
q[group#cym]= 0.06764307618141174
q[group#asm]= 0.06457653641700745
q[group#urd]= 0.06518526375293732
q[group#ben]= 0.06503463536500931
[0, 0, 0, 0, 0, 0, 0]
normalized dro_q:
q[group#slk]= 0.06278964877128601
q[group#swa]= 0.06552605330944061
q[group#nld]= 0.07159680128097534
q[group#deu]= 0.0721736028790474
q[group#kir]= 0.0636504590511322
q[group#fra]= 0.07539956271648407
q[group#lao]= 0.0649154931306839
q[group#hrv]= 0.06462496519088745
q[group#myv]= 0.06369995325803757
q[group#pol]= 0.06681555509567261
q[group#ita]= 0.06637180596590042
q[group#cym]= 0.06764188408851624
q[group#asm]= 0.06457585096359253
q[group#urd]= 0.06518447399139404
q[group#ben]= 0.06503387540578842
[0, 0, 0, 0, 0, 0, 0]
normalized dro_q:
q[group#slk]= 0.06281544268131256
q[group#swa]= 0.06552445888519287
q[group#nld]= 0.07159413397312164
q[group#deu]= 0.07217083126306534
q[group#kir]= 0.06364919990301132
q[group#fra]= 0.07539622485637665
q[group#lao]= 0.06491401046514511
q[group#hrv]= 0.06462353467941284
q[group#myv]= 0.06369868665933609
q[group#pol]= 0.06681373715400696
q[group#ita]= 0.06637006253004074
q[group#cym]= 0.06763991713523865
q[group#asm]= 0.06457442790269852
q[group#urd]= 0.06518293917179108
q[group#ben]= 0.06503237038850784
[5, 5, 5, 5]
normalized dro_q:
q[group#slk]= 0.06281109154224396
q[group#swa]= 0.06551948934793472
q[group#nld]= 0.07158777862787247
q[group#deu]= 0.07216434925794601
q[group#kir]= 0.0636446624994278
q[group#fra]= 0.07546745985746384
q[group#lao]= 0.0649091824889183
q[group#hrv]= 0.06461877375841141
q[group#myv]= 0.06369413435459137
q[group#pol]= 0.06680847704410553
q[group#ita]= 0.06636490672826767
q[group#cym]= 0.0676344707608223
q[group#asm]= 0.06456968188285828
q[group#urd]= 0.0651780515909195
q[group#ben]= 0.06502752006053925
[0, 0, 0, 0, 0, 0]
normalized dro_q:
q[group#slk]= 0.06283555179834366
q[group#swa]= 0.06551799178123474
q[group#nld]= 0.07158521562814713
q[group#deu]= 0.07216168195009232
q[group#kir]= 0.06364349275827408
q[group#fra]= 0.07546421885490417
q[group#lao]= 0.06490778923034668
q[group#hrv]= 0.06461743265390396
q[group#myv]= 0.06369295716285706
q[group#pol]= 0.06680675595998764
q[group#ita]= 0.06636326014995575
q[group#cym]= 0.06763260066509247
q[group#asm]= 0.06456834822893143
q[group#urd]= 0.06517661362886429
q[group#ben]= 0.06502611190080643
[0, 0, 0, 0, 0]
normalized dro_q:
q[group#slk]= 0.06286092847585678
q[group#swa]= 0.06551643460988998
q[group#nld]= 0.07158258557319641
q[group#deu]= 0.07215894758701324
q[group#kir]= 0.06364226341247559
q[group#fra]= 0.07546090334653854
q[group#lao]= 0.06490633636713028
q[group#hrv]= 0.06461603194475174
q[group#myv]= 0.06369172036647797
q[group#pol]= 0.06680496782064438
q[group#ita]= 0.06636155396699905
q[group#cym]= 0.06763067096471786
q[group#asm]= 0.0645669549703598
q[group#urd]= 0.0651751160621643
q[group#ben]= 0.06502463668584824
[13, 13, 13, 13, 13, 13, 13]
normalized dro_q:
q[group#slk]= 0.0628604143857956
q[group#swa]= 0.06551547348499298
q[group#nld]= 0.07158061116933823
q[group#deu]= 0.0721568763256073
q[group#kir]= 0.06364161521196365
q[group#fra]= 0.07545828074216843
q[group#lao]= 0.06490547955036163
q[group#hrv]= 0.06461522728204727
q[group#myv]= 0.06369106471538544
q[group#pol]= 0.06680379062891006
q[group#ita]= 0.06636045128107071
q[group#cym]= 0.0676293596625328
q[group#asm]= 0.06456615775823593
q[group#urd]= 0.06519143283367157
q[group#ben]= 0.0650237575173378
[3, 3, 3, 3]
normalized dro_q:
q[group#slk]= 0.06285951286554337
q[group#swa]= 0.06551411002874374
q[group#nld]= 0.07157819718122482
q[group#deu]= 0.07217777520418167
q[group#kir]= 0.06364057213068008
q[group#fra]= 0.0754551887512207
q[group#lao]= 0.06490422040224075
q[group#hrv]= 0.06461402028799057
q[group#myv]= 0.06369001418352127
q[group#pol]= 0.06680220365524292
q[group#ita]= 0.06635893881320953
q[group#cym]= 0.06762763112783432
q[group#asm]= 0.06456495821475983
q[group#urd]= 0.06519012153148651
q[group#ben]= 0.06502247601747513
[0]
normalized dro_q:
q[group#slk]= 0.06287392973899841
q[group#swa]= 0.06551331281661987
q[group#nld]= 0.07157640159130096
q[group#deu]= 0.07217588275671005
q[group#kir]= 0.06364008784294128
q[group#fra]= 0.0754527598619461
q[group#lao]= 0.06490352749824524
q[group#hrv]= 0.06461337208747864
q[group#myv]= 0.06368952244520187
q[group#pol]= 0.06680119782686234
q[group#ita]= 0.06635800749063492
q[group#cym]= 0.06762649118900299
q[group#asm]= 0.06456431746482849
q[group#urd]= 0.06518938392400742
q[group#ben]= 0.06502176076173782
[1, 1, 1, 1, 1, 1]
normalized dro_q:
q[group#slk]= 0.06287363171577454
q[group#swa]= 0.0655265673995018
q[group#nld]= 0.07157468050718307
q[group#deu]= 0.0721740573644638
q[group#kir]= 0.06363966315984726
q[group#fra]= 0.07545039802789688
q[group#lao]= 0.0649028941988945
q[group#hrv]= 0.06461279094219208
q[group#myv]= 0.06368909031152725
q[group#pol]= 0.06680025160312653
q[group#ita]= 0.06635713577270508
q[group#cym]= 0.06762541085481644
q[group#asm]= 0.06456374377012253
q[group#urd]= 0.0651887059211731
q[group#ben]= 0.06502111256122589
[6, 6, 6]
normalized dro_q:
q[group#slk]= 0.06287210434675217
q[group#swa]= 0.06552455574274063
q[group#nld]= 0.07157155871391296
q[group#deu]= 0.07217082381248474
q[group#kir]= 0.06363799422979355
q[group#fra]= 0.07544656097888947
q[group#lao]= 0.06493424624204636
q[group#hrv]= 0.06461094319820404
q[group#myv]= 0.06368741393089294
q[group#pol]= 0.06679800152778625
q[group#ita]= 0.06635496765375137
q[group#cym]= 0.06762301176786423
q[group#asm]= 0.06456190347671509
q[group#urd]= 0.0651867538690567
q[group#ben]= 0.06501919031143188
[13, 13, 13, 13, 13, 13]
normalized dro_q:
q[group#slk]= 0.06287078559398651
q[group#swa]= 0.06552276015281677
q[group#nld]= 0.07156867533922195
q[group#deu]= 0.07216782867908478
q[group#kir]= 0.06363654136657715
q[group#fra]= 0.07544297724962234
q[group#lao]= 0.06493255496025085
q[group#hrv]= 0.06460931152105331
q[group#myv]= 0.06368594616651535
q[group#pol]= 0.06679597496986389
q[group#ita]= 0.06635302305221558
q[group#cym]= 0.06762083619832993
q[group#asm]= 0.06456027925014496
q[group#urd]= 0.06521498411893845
q[group#ben]= 0.06501748412847519
[3, 3, 3]
normalized dro_q:
q[group#slk]= 0.06286798417568207
q[group#swa]= 0.06551941484212875
q[group#nld]= 0.07156410068273544
q[group#deu]= 0.07221674919128418
q[group#kir]= 0.06363358348608017
q[group#fra]= 0.07543761283159256
q[group#lao]= 0.06492933630943298
q[group#hrv]= 0.06460615247488022
q[group#myv]= 0.06368298083543777
q[group#pol]= 0.06679237633943558
q[group#ita]= 0.06634951382875443
q[group#cym]= 0.06761706620454788
q[group#asm]= 0.06455713510513306
q[group#urd]= 0.0652117058634758
q[group#ben]= 0.06501424312591553
[3]
normalized dro_q:
q[group#slk]= 0.06286588311195374
q[group#swa]= 0.06551679968833923
q[group#nld]= 0.0715603232383728
q[group#deu]= 0.07225546985864639
q[group#kir]= 0.0636313334107399
q[group#fra]= 0.07543309032917023
q[group#lao]= 0.06492683291435242
q[group#hrv]= 0.06460371613502502
q[group#myv]= 0.06368071585893631
q[group#pol]= 0.06678951531648636
q[group#ita]= 0.06634674221277237
q[group#cym]= 0.06761404871940613
q[group#asm]= 0.06455470621585846
q[group#urd]= 0.06520915031433105
q[group#ben]= 0.06501172482967377
[1, 1, 1, 1, 1]
normalized dro_q:
q[group#slk]= 0.06286562234163284
q[group#swa]= 0.06552931666374207
q[group#nld]= 0.0715586468577385
q[group#deu]= 0.07225367426872253
q[group#kir]= 0.06363094598054886
q[group#fra]= 0.07543078064918518
q[group#lao]= 0.06492623686790466
q[group#hrv]= 0.06460317224264145
q[group#myv]= 0.06368032097816467
q[group#pol]= 0.06678861379623413
q[group#ita]= 0.06634591519832611
q[group#cym]= 0.06761301308870316
q[group#asm]= 0.06455416977405548
q[group#urd]= 0.06520850956439972
q[group#ben]= 0.06501111388206482
[1, 1, 1, 1, 1]
normalized dro_q:
q[group#slk]= 0.06286488473415375
q[group#swa]= 0.06554891169071198
q[group#nld]= 0.07155641913414001
q[group#deu]= 0.0722513273358345
q[group#kir]= 0.06363007426261902
q[group#fra]= 0.07542789727449417
q[group#lao]= 0.06492514163255692
q[group#hrv]= 0.06460213661193848
q[group#myv]= 0.06367944180965424
q[group#pol]= 0.06678720563650131
q[group#ita]= 0.06634458154439926
q[group#cym]= 0.067611463367939
q[group#asm]= 0.0645531415939331
q[group#urd]= 0.06520736962556839
q[group#ben]= 0.06501000374555588
[1, 1, 1, 1, 1]
normalized dro_q:
q[group#slk]= 0.06286471337080002
q[group#swa]= 0.06556013226509094
q[group#nld]= 0.07155483961105347
q[group#deu]= 0.072249636054039
q[group#kir]= 0.06362977623939514
q[group#fra]= 0.07542569190263748
q[group#lao]= 0.06492463499307632
q[group#hrv]= 0.06460168212652206
q[group#myv]= 0.06367913633584976
q[group#pol]= 0.06678640097379684
q[group#ita]= 0.06634385138750076
q[group#cym]= 0.06761052459478378
q[group#asm]= 0.06455269455909729
q[group#urd]= 0.06520681828260422
q[group#ben]= 0.06500948965549469
[6, 6, 6]
normalized dro_q:
q[group#slk]= 0.06286276131868362
q[group#swa]= 0.06555766612291336
q[group#nld]= 0.07155123353004456
q[group#deu]= 0.07224589586257935
q[group#kir]= 0.06362767517566681
q[group#fra]= 0.07542134821414948
q[group#lao]= 0.06496252119541168
q[group#hrv]= 0.06459939479827881
q[group#myv]= 0.06367702782154083
q[group#pol]= 0.06678370386362076
q[group#ita]= 0.06634123623371124
q[group#cym]= 0.06760767102241516
q[group#asm]= 0.06455042213201523
q[group#urd]= 0.06520441919565201
q[group#ben]= 0.06500712782144547
[1, 1, 1, 1]
normalized dro_q:
q[group#slk]= 0.06286197155714035
q[group#swa]= 0.0655779093503952
q[group#nld]= 0.0715489536523819
q[group#deu]= 0.07224349677562714
q[group#kir]= 0.06362675130367279
q[group#fra]= 0.0754183977842331
q[group#lao]= 0.06496136635541916
q[group#hrv]= 0.06459830701351166
q[group#myv]= 0.06367609649896622
q[group#pol]= 0.06678223609924316
q[group#ita]= 0.06633985042572021
q[group#cym]= 0.06760606169700623
q[group#asm]= 0.06454934179782867
q[group#urd]= 0.0652032271027565
q[group#ben]= 0.06500596553087234
[14, 14]
normalized dro_q:
q[group#slk]= 0.06286104768514633
q[group#swa]= 0.06557650864124298
q[group#nld]= 0.0715465173125267
q[group#deu]= 0.07224094122648239
q[group#kir]= 0.06362569332122803
q[group#fra]= 0.07541529089212418
q[group#lao]= 0.06496007740497589
q[group#hrv]= 0.06459707766771317
q[group#myv]= 0.06367503106594086
q[group#pol]= 0.06678062677383423
q[group#ita]= 0.06633831560611725
q[group#cym]= 0.06760431081056595
q[group#asm]= 0.06454811990261078
q[group#urd]= 0.06520189344882965
q[group#ben]= 0.0650286003947258
[13, 13, 13, 13, 13, 13]
normalized dro_q:
q[group#slk]= 0.062860406935215
q[group#swa]= 0.06557540595531464
q[group#nld]= 0.07154440879821777
q[group#deu]= 0.07223871350288391
q[group#kir]= 0.06362491846084595
q[group#fra]= 0.07541252672672272
q[group#lao]= 0.0649590790271759
q[group#hrv]= 0.06459613889455795
q[group#myv]= 0.06367424875497818
q[group#pol]= 0.06677932292222977
q[group#ita]= 0.06633708626031876
q[group#cym]= 0.06760286539793015
q[group#asm]= 0.06454718858003616
q[group#urd]= 0.06522016227245331
q[group#ben]= 0.06502759456634521
[13, 13]
normalized dro_q:
q[group#slk]= 0.06285969913005829
q[group#swa]= 0.06557423621416092
q[group#nld]= 0.07154221832752228
q[group#deu]= 0.07223640382289886
q[group#kir]= 0.0636240765452385
q[group#fra]= 0.07540968060493469
q[group#lao]= 0.06495801359415054
q[group#hrv]= 0.06459513306617737
q[group#myv]= 0.06367339938879013
q[group#pol]= 0.06677794456481934
q[group#ita]= 0.0663357824087143
q[group#cym]= 0.06760134547948837
q[group#asm]= 0.06454619020223618
q[group#urd]= 0.0652393251657486
q[group#ben]= 0.06502651423215866
[5, 5, 5]
normalized dro_q:
q[group#slk]= 0.06285606324672699
q[group#swa]= 0.06557001173496246
q[group#nld]= 0.07153669744729996
q[group#deu]= 0.0722307339310646
q[group#kir]= 0.06362026929855347
q[group#fra]= 0.07547031342983246
q[group#lao]= 0.06495392322540283
q[group#hrv]= 0.06459111720323563
q[group#myv]= 0.0636695846915245
q[group#pol]= 0.06677345931529999
q[group#ita]= 0.06633138656616211
q[group#cym]= 0.0675966814160347
q[group#asm]= 0.06454218924045563
q[group#urd]= 0.06523516774177551
q[group#ben]= 0.06502240896224976
[5, 5, 5]
normalized dro_q:
q[group#slk]= 0.06285210698843002
q[group#swa]= 0.06556545197963715
q[group#nld]= 0.07153081148862839
q[group#deu]= 0.0722246915102005
q[group#kir]= 0.06361614167690277
q[group#fra]= 0.07553571462631226
q[group#lao]= 0.06494949758052826
q[group#hrv]= 0.06458677351474762
q[group#myv]= 0.06366544216871262
q[group#pol]= 0.06676863133907318
q[group#ita]= 0.06632665544748306
q[group#cym]= 0.06759166717529297
q[group#asm]= 0.06453785300254822
q[group#urd]= 0.06523068249225616
q[group#ben]= 0.06501796841621399
[5, 5, 5]
normalized dro_q:
q[group#slk]= 0.06284952163696289
q[group#swa]= 0.06556232273578644
q[group#nld]= 0.07152648270130157
q[group#deu]= 0.07222022861242294
q[group#kir]= 0.0636133998632431
q[group#fra]= 0.07558082789182663
q[group#lao]= 0.0649464875459671
q[group#hrv]= 0.06458383798599243
q[group#myv]= 0.06366269290447235
q[group#pol]= 0.06676525622606277
q[group#ita]= 0.06632336974143982
q[group#cym]= 0.06758812814950943
q[group#asm]= 0.06453492492437363
q[group#urd]= 0.06522762030363083
q[group#ben]= 0.06501494348049164
[5]
normalized dro_q:
q[group#slk]= 0.06284328550100327
q[group#swa]= 0.0655553862452507
q[group#nld]= 0.07151800394058228
q[group#deu]= 0.07221157103776932
q[group#kir]= 0.06360697001218796
q[group#fra]= 0.07567957788705826
q[group#lao]= 0.0649397075176239
q[group#hrv]= 0.06457715481519699
q[group#myv]= 0.06365624815225601
q[group#pol]= 0.06675800681114197
q[group#ita]= 0.06631623953580856
q[group#cym]= 0.06758067011833191
q[group#asm]= 0.06452825665473938
q[group#urd]= 0.06522077322006226
q[group#ben]= 0.06500814855098724
[7, 7, 7, 7, 7]
normalized dro_q:
q[group#slk]= 0.06284274905920029
q[group#swa]= 0.06555439531803131
q[group#nld]= 0.0715160146355629
q[group#deu]= 0.07220946997404099
q[group#kir]= 0.06360630691051483
q[group#fra]= 0.07567689567804337
q[group#lao]= 0.06493882089853287
q[group#hrv]= 0.06459388881921768
q[group#myv]= 0.06365557760000229
q[group#pol]= 0.06675681471824646
q[group#ita]= 0.06631512194871902
q[group#cym]= 0.06757934391498566
q[group#asm]= 0.06452743709087372
q[group#urd]= 0.06521984189748764
q[group#ben]= 0.06500725448131561
[7, 7, 7, 7]
normalized dro_q:
q[group#slk]= 0.0628422200679779
q[group#swa]= 0.06555341184139252
q[group#nld]= 0.07151403278112411
q[group#deu]= 0.07220737636089325
q[group#kir]= 0.0636056512594223
q[group#fra]= 0.07567422091960907
q[group#lao]= 0.06493794173002243
q[group#hrv]= 0.06461068987846375
q[group#myv]= 0.06365491449832916
q[group#pol]= 0.06675563007593155
q[group#ita]= 0.06631401181221008
q[group#cym]= 0.06757802516222
q[group#asm]= 0.06452662497758865
q[group#urd]= 0.06521891802549362
q[group#ben]= 0.06500636786222458
[7, 7, 7, 7]
normalized dro_q:
q[group#slk]= 0.0628417506814003
q[group#swa]= 0.0655524954199791
q[group#nld]= 0.0715121254324913
q[group#deu]= 0.07220534980297089
q[group#kir]= 0.06360505521297455
q[group#fra]= 0.07567162066698074
q[group#lao]= 0.06493712961673737
q[group#hrv]= 0.06462646275758743
q[group#myv]= 0.06365431100130081
q[group#pol]= 0.06675451248884201
q[group#ita]= 0.06631296873092651
q[group#cym]= 0.06757677346467972
q[group#asm]= 0.06452587991952896
q[group#urd]= 0.06521805375814438
q[group#ben]= 0.06500554084777832
[7, 7, 7, 7]
normalized dro_q:
q[group#slk]= 0.06284081190824509
q[group#swa]= 0.06555108726024628
q[group#nld]= 0.07150968164205551
q[group#deu]= 0.07220277935266495
q[group#kir]= 0.06360398232936859
q[group#fra]= 0.07566844671964645
q[group#lao]= 0.0649358257651329
q[group#hrv]= 0.06464925408363342
q[group#myv]= 0.06365323066711426
q[group#pol]= 0.06675289571285248
q[group#ita]= 0.06631142646074295
q[group#cym]= 0.06757500767707825
q[group#asm]= 0.06452465057373047
q[group#urd]= 0.06521670520305634
q[group#ben]= 0.06500422954559326
[6]
normalized dro_q:
q[group#slk]= 0.06283996254205704
q[group#swa]= 0.06554976850748062
q[group#nld]= 0.07150733470916748
q[group#deu]= 0.07220031321048737
q[group#kir]= 0.0636029988527298
q[group#fra]= 0.07566538453102112
q[group#lao]= 0.06495725363492966
q[group#hrv]= 0.0646480917930603
q[group#myv]= 0.06365223973989487
q[group#pol]= 0.0667513757944107
q[group#ita]= 0.06630998104810715
q[group#cym]= 0.06757333874702454
q[group#asm]= 0.06452351063489914
q[group#urd]= 0.06521544605493546
q[group#ben]= 0.06500300765037537
[7, 7, 7, 7]
normalized dro_q:
q[group#slk]= 0.06283888965845108
q[group#swa]= 0.06554821878671646
q[group#nld]= 0.07150473445653915
q[group#deu]= 0.07219759374856949
q[group#kir]= 0.0636017918586731
q[group#fra]= 0.0756620541214943
q[group#lao]= 0.06495580822229385
q[group#hrv]= 0.06467299908399582
q[group#myv]= 0.06365102529525757
q[group#pol]= 0.06674961745738983
q[group#ita]= 0.06630829721689224
q[group#cym]= 0.06757143139839172
q[group#asm]= 0.0645221397280693
q[group#urd]= 0.06521395593881607
q[group#ben]= 0.06500155478715897
[7, 7, 7, 7]
normalized dro_q:
q[group#slk]= 0.06283804774284363
q[group#swa]= 0.0655469074845314
q[group#nld]= 0.07150240242481232
q[group#deu]= 0.0721951425075531
q[group#kir]= 0.0636008232831955
q[group#fra]= 0.07565899938344955
q[group#lao]= 0.06495460122823715
q[group#hrv]= 0.0646943747997284
q[group#myv]= 0.06365004181861877
q[group#pol]= 0.06674810498952866
q[group#ita]= 0.06630685925483704
q[group#cym]= 0.0675697773694992
q[group#asm]= 0.06452100723981857
q[group#urd]= 0.06521270424127579
q[group#ben]= 0.06500034034252167
[7, 7, 7]
normalized dro_q:
q[group#slk]= 0.06283700466156006
q[group#swa]= 0.06554538756608963
q[group#nld]= 0.07149983942508698
q[group#deu]= 0.07219245284795761
q[group#kir]= 0.06359964609146118
q[group#fra]= 0.07565570622682571
q[group#lao]= 0.06495318561792374
q[group#hrv]= 0.06471873819828033
q[group#myv]= 0.06364885717630386
q[group#pol]= 0.06674637645483017
q[group#ita]= 0.06630520522594452
q[group#cym]= 0.06756789982318878
q[group#asm]= 0.06451966613531113
q[group#urd]= 0.06521124392747879
q[group#ben]= 0.06499891728162766
[7, 7, 7]
normalized dro_q:
q[group#slk]= 0.06283674389123917
q[group#swa]= 0.06554467976093292
q[group#nld]= 0.07149816304445267
q[group#deu]= 0.07219066470861435
q[group#kir]= 0.06359925866127014
q[group#fra]= 0.07565335184335709
q[group#lao]= 0.06495257467031479
q[group#hrv]= 0.06473134458065033
q[group#myv]= 0.06364846229553223
q[group#pol]= 0.06674547493457794
q[group#ita]= 0.06630437821149826
q[group#cym]= 0.06756686419248581
q[group#asm]= 0.06451912969350815
q[group#urd]= 0.06521059572696686
q[group#ben]= 0.06499829888343811
[7]
normalized dro_q:
q[group#slk]= 0.06283596903085709
q[group#swa]= 0.06554344296455383
q[group#nld]= 0.0714959055185318
q[group#deu]= 0.07218828797340393
q[group#kir]= 0.06359835714101791
q[group#fra]= 0.07565037906169891
q[group#lao]= 0.06495144218206406
q[group#hrv]= 0.06475182622671127
q[group#myv]= 0.0636475533246994
q[group#pol]= 0.06674402952194214
q[group#ita]= 0.06630301475524902
q[group#cym]= 0.06756527721881866
q[group#asm]= 0.06451807171106339
q[group#urd]= 0.06520941853523254
q[group#ben]= 0.06499715894460678
[jagupard32] 2024-08-23 16:24:45,539 (trainer:365) INFO: 30epoch results: [train] iter_time=0.001, forward_time=0.239, loss_ctc=0.455, loss=0.455, backward_time=0.033, grad_norm=7.837, clip=93.590, loss_scale=1.000, optim_step_time=0.006, optim0_lr0=5.000e-04, train_time=2.492, time=12 minutes and 58.97 seconds, total_count=75000, gpu_max_cached_mem_GB=8.484, [valid] loss_ctc=4.623, cer_ctc=0.220, loss_att=nan, acc=nan, cer=nan, wer=nan, loss=4.623, time=41.55 seconds, total_count=6330, gpu_max_cached_mem_GB=8.484, [att_plot] time=1 minute and 0.62 seconds, total_count=0, gpu_max_cached_mem_GB=8.484
[jagupard32] 2024-08-23 16:25:25,586 (trainer:433) INFO: The best model has been updated: valid.loss
[jagupard32] 2024-08-23 16:25:26,456 (trainer:487) INFO: The model files were removed: /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013/asr_train_asr_xlsr_aleb_dro_0.001/28epoch.pth, /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013/asr_train_asr_xlsr_aleb_dro_0.001/29epoch.pth
[jagupard32] 2024-08-23 16:25:26,457 (trainer:505) INFO: The training was finished at 30 epochs 
[jagupard32] 2024-08-23 16:25:26,458 (average_nbest_models:69) INFO: Averaging 5best models: criterion="valid.loss": /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013/asr_train_asr_xlsr_aleb_dro_0.001/valid.loss.ave_5best.pth
Traceback (most recent call last):
  File "/nlp/scr/bartelds/miniconda3/envs/asr-dro/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/nlp/scr/bartelds/miniconda3/envs/asr-dro/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/juice2/scr2/bartelds/git/asr-dro/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/juice2/scr2/bartelds/git/asr-dro/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/juice2/scr2/bartelds/git/asr-dro/espnet/espnet2/tasks/abs_task.py", line 1157, in main
    cls.main_worker(args)
  File "/juice2/scr2/bartelds/git/asr-dro/espnet/espnet2/tasks/abs_task.py", line 1487, in main_worker
    cls.trainer.run(
  File "/juice2/scr2/bartelds/git/asr-dro/espnet/espnet2/train/trainer.py", line 511, in run
    average_nbest_models(
  File "/nlp/scr/bartelds/miniconda3/envs/asr-dro/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/juice2/scr2/bartelds/git/asr-dro/espnet/espnet2/main_funcs/average_nbest_models.py", line 77, in average_nbest_models
    _loaded[e] = torch.load(
  File "/nlp/scr/bartelds/miniconda3/envs/asr-dro/lib/python3.9/site-packages/torch/serialization.py", line 986, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/nlp/scr/bartelds/miniconda3/envs/asr-dro/lib/python3.9/site-packages/torch/serialization.py", line 435, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/nlp/scr/bartelds/miniconda3/envs/asr-dro/lib/python3.9/site-packages/torch/serialization.py", line 416, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013/asr_train_asr_xlsr_aleb_dro_0.001/24epoch.pth'
wandb: - 126.741 MB of 170.125 MB uploaded
wandb: \ 127.636 MB of 170.125 MB uploaded
wandb: | 146.636 MB of 170.125 MB uploaded
wandb: / 170.125 MB of 170.125 MB uploaded
wandb: 
wandb: Run history:
wandb:                                        epoch ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███
wandb:                                    iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███
wandb: metrics/att_plot_gpu_max_cached_mem_GB_epoch ▁█████████████████████████████
wandb:                        metrics/backward_time █▅▂▃▁▃▂▃▁▁▃▃▃▃▃▄▃▂▄▃▃▃▃▂▂▁▃▂▂▅▃▃▂▁▂▂▃▁▁▂
wandb:                                 metrics/clip ████▇▆▄▅▅▄▃▁▃▂▃▂▂▅▂▃▁▃▂▅▃▆▃▅▄▃▃▆▅▄▅▅▅▇▅▅
wandb:                         metrics/forward_time ██▆▆▄▇██▃▂▄▄▅▆▅▇▄▁▆▅▄▅▅▅▃▂▄▄▅▅▅▄▄▄▄▅▅▃▁▄
wandb:                            metrics/grad_norm █▆▅▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▂▂▂
wandb:                            metrics/iter_time ▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▆▂
wandb:                                 metrics/loss █▇▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                             metrics/loss_ctc █▇▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                           metrics/loss_scale ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                           metrics/optim0_lr0 ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                      metrics/optim_step_time █▅▁▁▁▁▂▃▂▁▂▂▂▁▁▃▂▃▂▂▅▅▂▂▂▂▂▂▂▃▃▃▂▂▂▂▇▂▂▂
wandb:              train/train_backward_time_epoch █▃▃▄▃▅▂▅▅▃▃▅▃▆▅▄▄▃▂▄▃▂▄▄▂▂▃▅▁▁
wandb:                       train/train_clip_epoch ███▅▅▃▃▃▁▂▄▁▂▂▃▃▂▅▃▂▅▄▃▅▅▅▄▆▆▅
wandb:               train/train_forward_time_epoch █▆▇▇█▇▃▄▅▅▃▅▃▄▃▃▄▂▃▂▂▁▃▃▂▂▄▄▂▁
wandb:      train/train_gpu_max_cached_mem_GB_epoch ▁█████████████████████████████
wandb:                  train/train_grad_norm_epoch █▆▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▂▁▁▂▂
wandb:                  train/train_iter_time_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁█▁▁▁▁▁▁▂
wandb:                   train/train_loss_ctc_epoch █▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                       train/train_loss_epoch █▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 train/train_loss_scale_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                 train/train_optim0_lr0_epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:            train/train_optim_step_time_epoch ▄▂▁▁▂▃▂▃▂▂▂▂▂▃▃▅▃▂▃▃▃▂▃▃▂▂▂█▃▃
wandb:                             train/train_time ▇▇▅▅▃▅▅▅▃▁▂▃▄▄▄▅▃▁▅▃▃▃▃▃▂▁▃▃▃▃█▃▂▃▃▃▂▂▃▃
wandb:                 train/train_train_time_epoch ▇▅▆▅▆▅▃▃▄▄▂▃▂▅▂▂▃▁▂▁▁▁█▂▂▁▃▂▁▂
wandb:                    valid/valid_cer_ctc_epoch █▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:      valid/valid_gpu_max_cached_mem_GB_epoch ▁█████████████████████████████
wandb:                   valid/valid_loss_ctc_epoch █▄▃▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:                       valid/valid_loss_epoch █▄▃▂▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                                        epoch 30
wandb:                                    iteration 75000
wandb: metrics/att_plot_gpu_max_cached_mem_GB_epoch 8.48438
wandb:                        metrics/backward_time 0.0333
wandb:                                 metrics/clip 93.58974
wandb:                         metrics/forward_time 0.23856
wandb:                            metrics/grad_norm 7.83711
wandb:                            metrics/iter_time 0.00109
wandb:                                 metrics/loss 0.45543
wandb:                             metrics/loss_ctc 0.45543
wandb:                           metrics/loss_scale 1.0
wandb:                           metrics/optim0_lr0 0.0005
wandb:                      metrics/optim_step_time 0.00605
wandb:              train/train_backward_time_epoch 0.0333
wandb:                       train/train_clip_epoch 93.58974
wandb:               train/train_forward_time_epoch 0.23856
wandb:      train/train_gpu_max_cached_mem_GB_epoch 8.48438
wandb:                  train/train_grad_norm_epoch 7.83711
wandb:                  train/train_iter_time_epoch 0.00109
wandb:                   train/train_loss_ctc_epoch 0.45543
wandb:                       train/train_loss_epoch 0.45543
wandb:                 train/train_loss_scale_epoch 1.0
wandb:                 train/train_optim0_lr0_epoch 0.0005
wandb:            train/train_optim_step_time_epoch 0.00605
wandb:                             train/train_time 2.49192
wandb:                 train/train_train_time_epoch 2.49192
wandb:                        valid/valid_acc_epoch nan
wandb:                    valid/valid_cer_ctc_epoch 0.22007
wandb:                        valid/valid_cer_epoch nan
wandb:      valid/valid_gpu_max_cached_mem_GB_epoch 8.48438
wandb:                   valid/valid_loss_att_epoch nan
wandb:                   valid/valid_loss_ctc_epoch 4.62317
wandb:                       valid/valid_loss_epoch 4.62317
wandb:                        valid/valid_wer_epoch nan
wandb: 
wandb: 🚀 View run _juice2_scr2_bartelds_git_asr-dro_espnet_egs2_asr_dro_asr1 at: https://wandb.ai/bartelds/asr-dro-set-2-update/runs/voydxykj
wandb: ⭐️ View project at: https://wandb.ai/bartelds/asr-dro-set-2-update
wandb: Synced 6 W&B file(s), 2160 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: /nlp/scr/bartelds/git/asr-dro/espnet/egs2/asr_dro/asr1/exp/_exp_013/asr_train_asr_xlsr_aleb_dro_0.001/wandb/run-20240823_084025-voydxykj/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[3]
normalized dro_q:
q[group#slk]= 0.06283506751060486
q[group#swa]= 0.065542072057724
q[group#nld]= 0.07149350643157959
q[group#deu]= 0.07220903038978577
q[group#kir]= 0.06359732151031494
q[group#fra]= 0.0756472572684288
q[group#lao]= 0.06495017558336258
q[group#hrv]= 0.06475059688091278
q[group#myv]= 0.06364651024341583
q[group#pol]= 0.06674244999885559
q[group#ita]= 0.06630151718854904
q[group#cym]= 0.06756355613470078
q[group#asm]= 0.06451687961816788
q[group#urd]= 0.06520810723304749
q[group#ben]= 0.06499588489532471
[3]
normalized dro_q:
q[group#slk]= 0.06283041834831238
q[group#swa]= 0.06553678959608078
q[group#nld]= 0.07148683816194534
q[group#deu]= 0.07228533923625946
q[group#kir]= 0.06359249353408813
q[group#fra]= 0.07563962042331696
q[group#lao]= 0.0649450272321701
q[group#hrv]= 0.06474550068378448
q[group#myv]= 0.06364166736602783
q[group#pol]= 0.06673688441514969
q[group#ita]= 0.0662960559129715
q[group#cym]= 0.06755780428647995
q[group#asm]= 0.06451183557510376
q[group#urd]= 0.06520289927721024
q[group#ben]= 0.06499072909355164
[3]
normalized dro_q:
q[group#slk]= 0.06282988935709
q[group#swa]= 0.06553580611944199
q[group#nld]= 0.07148485630750656
q[group#deu]= 0.07230096310377121
q[group#kir]= 0.06359183043241501
q[group#fra]= 0.07563693821430206
q[group#lao]= 0.06494414061307907
q[group#hrv]= 0.06474465131759644
q[group#myv]= 0.06364099681377411
q[group#pol]= 0.06673569977283478
q[group#ita]= 0.06629494577646255
q[group#cym]= 0.0675564780831337
q[group#asm]= 0.0645110234618187
q[group#urd]= 0.06520196795463562
q[group#ben]= 0.06498983502388
# Accounting: time=27999 threads=1
# Ended (code 1) at Fri Aug 23 16:26:18 PDT 2024, elapsed time 27999 seconds

make: *** [exp013.mk:202: train_asr_xlsr_aleb_dro_0.001] Error 1
###############################
end time: 2024-08-23 16:26:20.342543
elapsed time: 7:46:43.282339
